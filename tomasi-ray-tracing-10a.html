<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Maurizio Tomasi maurizio.tomasi@unimi.it">
  <title>Lezione 10</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      /* overflow: visible; */
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/theme/white.css" id="theme">
  <link rel="stylesheet" href="./css/custom.css"/>
  <link rel="stylesheet" href="./css/asciinema-player.css"/>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Lezione 10</h1>
  <p class="subtitle">Path tracing</p>
  <p class="author">Maurizio Tomasi <a
href="mailto:maurizio.tomasi@unimi.it"
class="email">maurizio.tomasi@unimi.it</a></p>
</section>

<section id="path-tracing" class="slide level1">
<h1>Path tracing</h1>
</section>
<section id="equazione-del-rendering" class="slide level1">
<h1>Equazione del rendering</h1>
<ul>
<li><p>Per risolvere l’equazione del rendering dobbiamo tracciare il
percorso di raggi luminosi nello spazio tridimensionale e risolvere
l’equazione del rendering:</p>
<p><span class="math display">
\begin{aligned}
L(x \rightarrow \Theta) = &amp;L_e(x \rightarrow \Theta) +\\
&amp;\int_{4\pi} f_r(x, \Psi \rightarrow \Theta)\,L(x \leftarrow
\Psi)\,\cos(N_x, \Psi)\,\mathrm{d}\omega_\Psi.
\end{aligned}
</span></p></li>
<li><p>Esistono vari modi per risolvere l’equazione, ma noi ne
implementeremo solo due: il path tracing e il point-light
tracing.</p></li>
</ul>
</section>
<section id="path-tracing-1" class="slide level1">
<h1>Path tracing</h1>
<ul>
<li><p>È il metodo concettualmente più semplice da
implementare.</p></li>
<li><p>È in grado di produrre soluzioni <em>esatte</em>, nel senso che
sono <em>unbiased</em> (senza effetti sistematici).</p></li>
<li><p>È computazionalmente molto inefficiente.</p></li>
<li><p>Ci sono molte tecniche per renderlo più veloce; noi
implementeremo solo le più semplici (non è l’obbiettivo principale del
corso!).</p></li>
</ul>
</section>
<section id="path-tracing-2" class="slide level1">
<h1>Path tracing</h1>
<ul>
<li><p>Il termine più complesso dell’equazione del rendering è
l’integrale</p>
<p><span class="math display">
\begin{aligned}
L(x \rightarrow \Theta) = &amp;L_e(x \rightarrow \Theta) +\\
&amp;\int_{2\pi} f_r(x, \Psi \rightarrow \Theta)\,L(x \leftarrow
\Psi)\,\cos(N_x, \Psi)\,\mathrm{d}\omega_\Psi,
\end{aligned}
</span></p>
<p>che per semplicità consideriamo applicato a un materiale
<em>opaco</em> e non trasparente (4π→2π).</p></li>
<li><p>Come abbiamo visto, esso è in realtà un integrale multiplo, su un
numero arbitrario di dimensioni: in questi casi, gli integrali si
stimano efficientemente usando metodi Monte Carlo (MC).</p></li>
</ul>
</section>
<section id="path-tracing-e-mc" class="slide level1">
<h1>Path tracing e MC</h1>
<ul>
<li><p>L’algoritmo di <em>path tracing</em> è il seguente:</p>
<ol>
<li>Per ogni pixel dello schermo si proietta un raggio attraverso il
pixel.</li>
<li>Quando un raggio colpisce una superficie in un punto <span
class="math inline">P</span>, si valuta l’emissione <span
class="math inline">L_e</span> al punto <span
class="math inline">P</span>.</li>
<li>Si valuta l’integrale sul punto <span class="math inline">P</span>
usando il metodo Monte Carlo, campionando l’angolo solido con <span
class="math inline">N</span> nuovi raggi secondari che dipartono da
<span class="math inline">P</span> lungo direzioni casuali.</li>
<li>Per ogni raggio secondario si procede ricorsivamente.</li>
</ol></li>
<li><p>L’algoritmo consente di ottenere una soluzione esatta per
l’equazione del rendering, sebbene al prezzo di un grande tempo di
calcolo.</p></li>
</ul>
</section>
<section class="slide level1">

<iframe width="1050" height="590" src="https://www.youtube.com/embed/eo_MTI-d28s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p><a href="https://www.youtube.com/watch?v=eo_MTI-d28s">An explanation
of the rendering equation (Eric Arnebäck)</a></p>
</section>
<section id="probabilità-e-monte-carlo" class="slide level1">
<h1>Probabilità e Monte Carlo</h1>
</section>
<section id="ripasso-di-probabilità" class="slide level1">
<h1>Ripasso di probabilità</h1>
<ul>
<li><p>Data una variabile <span class="math inline">X</span>, la
<em>funzione di distribuzione cumulativa</em> (CDF) <span
class="math inline">P(x)</span> è la probabilità che <span
class="math inline">X</span> sia inferiore ad un valore <span
class="math inline">x</span> fissato:</p>
<p><span class="math display">
P(x) = \text{Pr}\{X \leq x\}
</span></p></li>
<li><p>La <em>funzione di densità di probabilità</em> (PDF) <span
class="math inline">p(x)</span> è la derivata di <span
class="math inline">P(x)</span>, ed è tale che <span
class="math inline">p(x)\,\mathrm{d}x</span> è la probabilità che <span
class="math inline">X</span> stia nell’intervallo <span
class="math inline">[x, x + \mathrm{d}x]</span>:</p>
<p><span class="math display">
p(x) = \frac{\mathrm{d}P(x)}{\mathrm{d}x}.
</span></p></li>
</ul>
</section>
<section id="ripasso-di-probabilità-1" class="slide level1">
<h1>Ripasso di probabilità</h1>
<ul>
<li><p>Siccome <span class="math inline">\lim_{x \rightarrow -\infty}
P(x) = 0</span> e <span class="math inline">\lim_{x \rightarrow +\infty}
P(x) = 1</span>, ne segue che</p>
<p><span class="math display">
\int_{\mathbb{R}} p(x)\,\mathrm{d}x = 1.
</span></p></li>
<li><p>Dalla definizione segue che la probabilità che <span
class="math inline">X</span> cada nell’intervallo <span
class="math inline">[a, b]</span> è</p>
<p><span class="math display">
P\bigl(X \in [a, b]\bigr) = P(b) - P(a) = \int_a^b p(x)\,\mathrm{d}x.
</span></p></li>
</ul>
</section>
<section id="ripasso-di-probabilità-2" class="slide level1">
<h1>Ripasso di probabilità</h1>
<ul>
<li><p>Si definisce <em>valore di aspettazione</em> di una funzione
<span class="math inline">f(X)</span> dipendente dalla variabile casuale
<span class="math inline">X</span> con PDF <span
class="math inline">p(x)</span> il valore</p>
<p><span class="math display">
E_p[f] = \int_\mathbb{R}\,f(x)\,p(x)\,\mathrm{d}x.
</span></p></li>
<li><p>Si definisce <em>varianza</em> di <span
class="math inline">f(X)</span> rispetto a <span
class="math inline">p</span> il valore</p>
<p><span class="math display">
V_p[f] = E_p\left[\bigl(f(x) - E_p[f]\bigr)^2\right] =
\int_\mathbb{R}\,\bigl(f(x) - E_p[f]\bigr)^2\,p(x)\,\mathrm{d}x.
</span></p></li>
<li><p>La <em>deviazione standard</em> è definita come <span
class="math inline">\sqrt{V_p[f]}</span>.</p></li>
</ul>
</section>
<section id="ripasso-di-probabilità-3" class="slide level1">
<h1>Ripasso di probabilità</h1>
<ul>
<li><p><span class="math inline">E_p</span> è un operatore lineare:</p>
<p><span class="math display">
E_p[a f(x)] = a E_p[f(x)], \quad E[f(x) + g(x)] = E[f(x)] + E[g(x)].
</span></p></li>
<li><p>Per la varianza (che <strong>non è lineare!</strong>) vale invece
che</p>
<p><span class="math display">
V_p[a f(x)] = a^2 V_p[f(x)].
</span></p></li>
<li><p>Da queste proprietà si ricava che</p>
<p><span class="math display">
V_p[f] = E_p\bigl[f^2(x)\bigr] - E_p\bigl[f(x)\bigr]^2.
</span></p></li>
</ul>
</section>
<section id="integrazione-monte-carlo" class="slide level1">
<h1>Integrazione Monte Carlo</h1>
</section>
<section id="integrazione-monte-carlo-1" class="slide level1">
<h1>Integrazione Monte Carlo</h1>
<ul>
<li><p>Nell’algoritmo di <em>path tracing</em> si calcolano gli
integrali con metodi Monte Carlo</p></li>
<li><p>Ciò richiede la generazione di numeri pseudo-casuali</p></li>
<li><p>Siccome si tratta di integrali molto complessi (infiniti!), è
spesso necessario implementare ottimizzazioni: una di queste
ottimizzazioni (<em>importance sampling</em>) richiede di estrarre
numeri pseudo-casuali con distribuzioni di probabilità
arbitrarie</p></li>
<li><p>Vediamo un esempio semplice di integrale Monte Carlo e di
<em>importance sampling</em></p></li>
</ul>
</section>
<section id="esempio" class="slide level1">
<h1>Esempio</h1>
<ul>
<li><p>Supponiamo di voler calcolare numericamente il valore
dell’integrale</p>
<p><span class="math display">
I = \int_a^b f(x)\,\mathrm{d}x
</span></p></li>
<li><p>Il metodo della media fornisce una semplice approssimazione <span
class="math inline">F_N</span> per <span
class="math inline">I</span>:</p>
<p><span class="math display">
I = \int_a^b f(x)\,\mathrm{d}x \approx F_N \equiv \frac{b - a}N
\sum_{i=1}^N f(X_i),
</span></p>
<p>dove <span class="math inline">x_i</span> sono <span
class="math inline">N</span> numeri casuali con PDF costante <span
class="math inline">p(x) = 1 / (b - a)</span>.</p></li>
</ul>
</section>
<section class="slide level1">

<p><span class="math display">
\begin{aligned}
E_p[F_N] &amp;= E_p\left[\frac{b - a}N \sum_{i = 1}^N f(x_i)\right] =\\
&amp;= \frac{b - a}N \sum_{i = 1}^N E_p[f(x_i)] =\\
&amp;= \frac{b - a}N \sum_{i = 1}^N \int_{\mathbb{R}}
f(x)\,p(x)\,\mathrm{d}x = \\
&amp;= \frac1N \sum_{i = 1}^N \int_a^b f(x)\,\mathrm{d}x = \int_a^b
f(x)\,\mathrm{d}x = I.
\end{aligned}
</span></p>
</section>
<section id="importance-sampling" class="slide level1">
<h1><em>Importance sampling</em></h1>
<ul>
<li><p>È facile dimostrare che se <span class="math inline">X</span>
segue una distribuzione arbitraria <span
class="math inline">p(x)</span>, un estimatore dell’integrale è</p>
<p><span class="math display">
F_N = \frac1N \sum_{i = 1}^N \frac{f(X_i)}{p(X_i)},
</span></p>
<p>a patto che <span class="math inline">p(x) &gt; 0</span> quando <span
class="math inline">f(x) \not= 0</span>. (Notate che qui non figura
<span class="math inline">b - a</span>).</p></li>
<li><p>Se si sceglie bene <span class="math inline">p(x)</span>, è
possibile aumentare l’accuratezza della stima. Nel caso in cui <span
class="math inline">f(x) = k \cdot p(x)</span> infatti, il termine nella
sommatoria è costante (k) e uguale all’integrale, quindi basta <span
class="math inline">N = 1</span> per stimarlo!</p></li>
</ul>
</section>
<section id="esempio-1" class="slide level1">
<h1>Esempio</h1>
<ul>
<li><p>Vediamo un esempio pratico nel calcolo dell’integrale di <span
class="math inline">f(x) = \sqrt{x}\,\sin x</span>:</p>
<p><span class="math display">
\int_0^\pi f(x)\,\mathrm{d}x = \int_0^\pi \sqrt{x}\,\sin x\,\mathrm{d}x
\approx 2.435.
</span></p></li>
<li><p>Per usare l’<em>importance sampling</em> dobbiamo decidere quale
<span class="math inline">p(x)</span> usare:</p>
<ol>
<li><span class="math inline">p(x) \propto \sqrt{x}\,\sin x</span>? (No,
questa è proprio l’integranda!)</li>
<li><span class="math inline">p(x) \propto \sqrt{x}</span>?</li>
<li><span class="math inline">p(x) \propto \sin x</span>?</li>
</ol></li>
<li><p>Per decidere è sempre bene fare il grafico dell’integranda <span
class="math inline">f(x)</span>.</p></li>
</ul>
</section>
<section class="slide level1">

<ul>
<li><p>Questo è il grafico di <span class="math inline">f(x)</span> e
delle nostre ipotesi per <span class="math inline">p(x)</span>:</p>
<center>
<p><img
data-src="pd-images/1433de1529e0045101dd2a5777ee7d806b8bd383.svg"
data-im_fname="importance-sampling-demo1" /></p>
</center></li>
<li><p>Chiaramente l’opzione migliore è <span class="math inline">p(x)
\propto \sin x</span>, perché la sua forma ricorda quella di <span
class="math inline">f(x)</span>.</p></li>
</ul>
</section>
<section id="esempio-2" class="slide level1">
<h1>Esempio</h1>
<ul>
<li><p>Normalizzando <span class="math inline">p(x) \propto \sin
x</span> sul dominio di integrazione si ottiene che</p>
<p><span class="math display">
p(x) = \frac12 \sin x\,\chi_{[0, \pi]}(x).
</span></p></li>
<li><p>Dobbiamo ora ottenere numeri casuali <span
class="math inline">X_i</span> che seguano questa distribuzione. Usiamo
il metodo della funzione inversa, passando dalla PDF <span
class="math inline">p(x)</span> alla CDF <span
class="math inline">P(x)</span>:</p>
<p><span class="math display">
P(x) = \int_{-\infty}^x p(x&#39;)\,\mathrm{d}x&#39; = \frac12(1 - \cos
x).
</span></p></li>
</ul>
</section>
<section id="esempio-3" class="slide level1">
<h1>Esempio</h1>
<ul>
<li><p>Siccome <span class="math inline">P(x) = \frac12 (1 - \cos
x)</span>, allora</p>
<p><span class="math display">
P^{-1}(y) = \arccos(1 - 2y).
</span></p></li>
<li><p>Se quindi <span class="math inline">Y_i</span> è distribuito
uniformemente su <span class="math inline">[0, 1]</span>, allora <span
class="math inline">P^{-1}(Y_i) = X_i</span> è distribuito secondo <span
class="math inline">p(x)</span>. (Vedremo tra poco come dimostrarlo
rigorosamente).</p></li>
<li><p>Implementiamo ora un codice Python che calcoli l’integrale col
metodo della media <strong>senza</strong> e <strong>con</strong>
l’<em>importance sampling</em>, per verificare effettivamente quale sia
il vantaggio.</p></li>
</ul>
</section>
<section class="slide level1">

<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np, matplotlib.pylab <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate(f, n):  <span class="co"># Plain mean method</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.random.rand(n) <span class="op">*</span> np.pi  <span class="co"># Random numbers in the range [0, π]</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(np.pi <span class="op">*</span> f(x))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_importance(f, n):  <span class="co"># Mean method with importance sampling</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.random.rand(n)       <span class="co"># Uniform random numbers in the range [0, 1]</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    xp <span class="op">=</span> np.arccos(<span class="dv">1</span> <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> x)   <span class="co"># These are distributed as p(x) = 1/2 sin(x)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(f(xp) <span class="op">/</span> (<span class="fl">0.5</span> <span class="op">*</span> np.sin(xp)))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate many times the same integral using the two methods</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: np.sqrt(x) <span class="op">*</span> np.sin(x)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>est1 <span class="op">=</span> [estimate(f, <span class="dv">1000</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>)]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>est2 <span class="op">=</span> [estimate_importance(f, <span class="dv">1000</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(est1))]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.plot(est1, label<span class="op">=</span><span class="st">&#39;$p(x) = k$ (plain method)&#39;</span>, color<span class="op">=</span><span class="st">&quot;gray&quot;</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.plot(est2, label<span class="op">=</span><span class="st">&#39;$p(x) \propto \sin x$ (importance sampling)&#39;</span>,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>         color<span class="op">=</span><span class="st">&quot;black&quot;</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="fl">2.435321164</span> <span class="op">*</span> np.ones(<span class="bu">len</span>(est1)), label<span class="op">=</span><span class="st">&quot;True value&quot;</span>,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>         color<span class="op">=</span><span class="st">&quot;red&quot;</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;# of the estimate&quot;</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Estimated value for the integral&quot;</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code></pre></div>
</section>
<section class="slide level1">

<center>
<img data-src="./media/importance-sampling-demo.svg" />
</center>
</section>
<section id="altra-possibilità" class="slide level1">
<h1>Altra possibilità</h1>
<ul>
<li><p>Cosa cambierebbe se scegliessimo <span class="math inline">p(x)
\propto \sqrt x</span>?</p></li>
<li><p>Potete verificare che il seguente codice Python implementa
l’<em>importance sampling</em> in questo caso:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_importance2(f, n):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.random.rand(n)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This is because P^−1 (y) = (3/2 y)^(2/3)</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    xp <span class="op">=</span> np.pi <span class="op">*</span> x <span class="op">**</span> (<span class="dv">2</span> <span class="op">/</span> <span class="dv">3</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># p(x) = 3/(2π^3/2) * √x</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(f(xp) <span class="op">/</span> (<span class="dv">3</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">**</span> <span class="fl">1.5</span>) <span class="op">*</span> np.sqrt(xp)))</span></code></pre></div></li>
<li><p>Questa volta usiamo un istogramma per mostrare meglio come le
stime Monte Carlo si distribuiscono attorno al valore vero.</p></li>
</ul>
</section>
<section class="slide level1">

<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The first part of the script is the same as in the previous example</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># …</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_importance2(f, n):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.random.rand(n)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># These are distributed as p(x) = 3/(2π^3/2) * √x</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    xp <span class="op">=</span> np.pi <span class="op">*</span> x<span class="op">**</span>(<span class="dv">2</span><span class="op">/</span><span class="dv">3</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(f(xp) <span class="op">/</span> (<span class="dv">3</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">**</span> <span class="fl">1.5</span>) <span class="op">*</span> np.sqrt(xp)))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>est0 <span class="op">=</span> [estimate(f, <span class="dv">1000</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10_000</span>)]</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>est1 <span class="op">=</span> [estimate_importance1(f, <span class="dv">1000</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10_000</span>)]</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>est2 <span class="op">=</span> [estimate_importance2(f, <span class="dv">1000</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10_000</span>)]</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>plt.hist(est0, bins<span class="op">=</span><span class="dv">50</span>, label<span class="op">=</span><span class="st">&quot;$p(x) = k$&quot;</span>, orientation<span class="op">=</span><span class="st">&quot;horizontal&quot;</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>plt.hist(est2, bins<span class="op">=</span><span class="dv">50</span>, label<span class="op">=</span><span class="st">&quot;$p(x) \propto \sqrt</span><span class="sc">{x}</span><span class="st">$&quot;</span>, orientation<span class="op">=</span><span class="st">&quot;horizontal&quot;</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>plt.hist(est1, bins<span class="op">=</span><span class="dv">50</span>, label<span class="op">=</span><span class="st">&quot;$p(x) \propto \sin x$&quot;</span>, orientation<span class="op">=</span><span class="st">&quot;horizontal&quot;</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;Number of estimates&quot;</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Estimated value for the integral&quot;</span>)</span></code></pre></div>
</section>
<section class="slide level1">

<center>
<img data-src="media/importance-sampling-demo2.svg" />
</center>
</section>
<section id="monte-carlo-e-ray-tracing" class="slide level1">
<h1>Monte Carlo e ray-tracing</h1>
</section>
<section id="applicazione-al-ray-tracing" class="slide level1">
<h1>Applicazione al ray-tracing</h1>
<ul>
<li><p>Consideriamo ora il problema dell’equazione del rendering:</p>
<p><span class="math display">
\begin{aligned}
L(x \rightarrow \Theta) = &amp;L_e(x \rightarrow \Theta) +\\
&amp;\int_{4\pi} f_r(x, \Psi \rightarrow \Theta)\,L(x \leftarrow
\Psi)\,\cos(N_x, \Psi)\,\mathrm{d}\omega_\Psi.
\end{aligned}
</span></p></li>
<li><p>L’integrale è su due dimensioni (<span
class="math inline">\mathrm{d}\omega</span> è un angolo solido), quindi
ha senso usare il metodo della media.</p></li>
</ul>
</section>
<section id="uso-del-metodo-della-media" class="slide level1">
<h1>Uso del metodo della media</h1>
<ul>
<li><p>Si può stimare l’integrale con questa procedura:</p>
<ol>
<li>Si scelgono <span class="math inline">N</span> direzioni casuali
<span class="math inline">\Psi_i</span> (o equivalentemente angoli
solidi infinitesimi <span class="math inline">\mathrm{d}
\omega_i</span>);</li>
<li>Si valuta l’integranda lungo le <span class="math inline">N</span>
direzioni, ottenendo <span class="math inline">N</span> stime;</li>
<li>Si applica il metodo della media calcolando la media di tutte le
<span class="math inline">N</span> stime.</li>
</ol></li>
<li><p>Ci sono però due complicazioni:</p>
<ul>
<li>Come si scelgono le «direzioni casuali» <span
class="math inline">\Psi_i</span>? Qui siamo in 2D, non in 1D!</li>
<li>Come si valuta l’integranda, visto che è ricorsiva?</li>
</ul></li>
</ul>
</section>
<section id="direzioni-casuali" class="slide level1">
<h1>Direzioni casuali</h1>
<ul>
<li><p>Abbiamo sempre indicato le direzioni con gli angoli θ e φ, legati
alle coordinate cartesiane tramite le relazioni sferiche con <span
class="math inline">r = 1</span>:</p>
<p><span class="math display">
x = \sin\theta\cos\varphi, \quad y = \sin\theta\sin\varphi, \quad z =
\cos\theta.
</span></p></li>
<li><p>Se anche volessimo applicare il metodo della media <em>senza</em>
importance sampling, dovremmo avere una probabilità <span
class="math inline">p(\omega)</span> costante. Ma questo
<strong>non</strong> coincide col chiedere che <span
class="math inline">p(\theta)</span> e <span
class="math inline">p(\varphi)</span> siano costanti!</p></li>
<li><p>Dobbiamo scegliere le direzioni casuali in modo che <span
class="math inline">p(\omega)</span> sia una costante, e per fare questo
dobbiamo capire come i cambi di variabile agiscono sulle distribuzioni
di probabilità in <span class="math inline">n</span>
dimensioni.</p></li>
</ul>
</section>
<section id="distribuzioni-di-probabilità-1d" class="slide level1">
<h1>Distribuzioni di probabilità 1D</h1>
<ul>
<li><p>Se le variabili casuali <span class="math inline">X_1, X_2,
\ldots</span> hanno distribuzione <span
class="math inline">p_X(x)</span>, e se definiamo una trasformazione
<em>invertibile</em> <span class="math inline">Y = f(X)</span>, per cui
esista quindi <span class="math inline">f^{-1}</span> tale che <span
class="math inline">X = f^{-1}(Y)</span>, ci chiediamo: qual è la
distribuzione <span class="math inline">p_Y(y)</span> degli <span
class="math inline">Y_i</span>?</p></li>
<li><p>Ci sono molti modi per calcolare <span
class="math inline">p_Y</span>. Il più semplice consiste nel calcolare
direttamente la CDF degli <span class="math inline">Y</span>:</p>
<p><span class="math display">
P_Y(y) = \mathrm{Pr}(Y \leq y) = \mathrm{Pr}\bigl(f(X) \leq y\bigr).
</span></p>
<p>A questo punto possiamo applicare <span
class="math inline">f^{-1}</span> ad entrambi i membri della
disequazione <span class="math inline">f(X) \leq y</span>, ma con
un’accortezza.</p></li>
</ul>
</section>
<section id="distribuzioni-di-probabilità-1d-1" class="slide level1">
<h1>Distribuzioni di probabilità 1D</h1>
<ul>
<li><p>Se <span class="math inline">f^{-1}</span> è una funzione
<em>crescente</em>, vale che</p>
<p><span class="math display">
f(X) \leq y\quad\Rightarrow\quad X \leq f^{-1}(y).
</span></p></li>
<li><p>Se invece è decrescente, vale che</p>
<p><span class="math display">
f(X) \leq y\quad\Rightarrow\quad X \geq f^{-1}(y).
</span></p></li>
<li><p>Vediamo innanzitutto il caso in cui <span
class="math inline">f^{-1}</span> è crescente.</p></li>
</ul>
</section>
<section id="caso-di-inversa-crescente" class="slide level1">
<h1>Caso di inversa crescente</h1>
<ul>
<li><p>In questo caso</p>
<p><span class="math display">
P_Y(y) = \text{Pr}\bigl(X \leq f^{-1}(y)\bigr) =
P_X\bigl(f^{-1}(y)\bigr).
</span></p></li>
<li><p>Dalla CDF <span class="math inline">P_Y(y)</span> possiamo
passare alla PDF applicando la formula per la derivata di una funzione
composta:</p>
<p><span class="math display">
p_Y(y) = P_Y&#39;(y) = \frac{\mathrm{d}
P_X\bigl(f^{-1}(y)\bigr)}{\mathrm{d}y} = p_X\bigl(f^{-1}(y)\bigr)\cdot
\frac{\mathrm{d}f^{-1}}{\mathrm{d}y}(y).
</span></p></li>
</ul>
</section>
<section id="caso-di-inversa-decrescente" class="slide level1">
<h1>Caso di inversa decrescente</h1>
<ul>
<li><p>Se <span class="math inline">f^{-1}</span> è una funzione
decrescente, allora vale che</p>
<p><span class="math display">
P_Y(y) = \text{Pr}\bigl(X \geq f^{-1}(y)\bigr) = 1 - \text{Pr}\bigl(X
\leq f^{-1}(y)\bigr) = 1 - P_X\bigl(f^{-1}(y)\bigr).
</span></p></li>
<li><p>Applicando di nuovo la derivata come nel caso precedente,
otteniamo che</p>
<p><span class="math display">
p_Y(y) = P_Y&#39;(y) = \frac{\mathrm{d}
P_X\bigl(f^{-1}(y)\bigr)}{\mathrm{d}y} = -p_X\bigl(f^{-1}(y)\bigr)\cdot
\frac{\mathrm{d}f^{-1}}{\mathrm{d}y}(y).
</span></p></li>
<li><p>Notiamo però che la derivata di <span
class="math inline">f^{-1}</span> in questo caso è
<em>negativa</em>.</p></li>
</ul>
</section>
<section id="caso-generale" class="slide level1">
<h1>Caso generale</h1>
<ul>
<li><p>Mettendo insieme i due casi, otteniamo che</p>
<p><span class="math display">
p_Y(y) = p_X\bigl(f^{-1}(y)\bigr)\cdot
\left|\frac{\mathrm{d}f^{-1}}{\mathrm{d}y}(y)\right|,
</span></p>
<p>che è corretto perché <span class="math inline">p_Y(y)</span> deve
sempre essere positiva. Questa relazione inoltre preserva la
normalizzazione di <span class="math inline">p_Y</span>.</p></li>
<li><p>Un trucco mnemonico per ricordarla parte dal fatto che deve
valere <span class="math inline">p_Y(y)\,\left|\mathrm{d}y\right| =
p_X(x)\,\left|\mathrm{d}x\right|</span>: da qui si ricava facilmente la
relazione sopra.</p></li>
<li><p>Vediamo ora qualche esempio pratico.</p></li>
</ul>
</section>
<section id="esempi" class="slide level1">
<h1>Esempi</h1>
<p>Supponiamo che le variabili <span class="math inline">X</span> siano
distribuite uniformemente su <span class="math inline">[0, 1]</span>, in
modo che <span class="math inline">p_X(x) = \chi_{[0, 1]}(x)</span>, e
supponiamo che <span class="math inline">Y = f(X)</span>. Di
conseguenza:</p>
<ol>
<li><p>Se <span class="math inline">f(X) = X + 1</span> e <span
class="math inline">f^{-1}(Y) = Y - 1</span>, allora <span
class="math inline">p_Y(y) = \chi_{[1, 2]}(y)</span>.</p></li>
<li><p>Se <span class="math inline">f(X) = 2X</span> e <span
class="math inline">f^{-1}(Y) = Y / 2</span>, allora <span
class="math inline">p_Y(y) = \frac12 \chi_{[0, 2]}(y)</span>.</p></li>
<li><p>Se <span class="math inline">f(X) = X^2</span> e <span
class="math inline">f^{-1}(Y) = \sqrt{Y}</span>, allora <span
class="math inline">p_Y(y) = \frac{\chi_{[0,
1]}(y)}{2\sqrt{y}}</span>.</p></li>
<li><p>Se <span class="math inline">f(X) = e^{X - 1}</span> e <span
class="math inline">f^{-1}(Y) = 1 + \log Y</span>, allora <span
class="math inline">p_Y(y) = \frac{\chi_{[1/e, 1]}(y)}y</span>.</p></li>
</ol>
</section>
<section id="verifica-in-python" class="slide level1">
<h1>Verifica in Python</h1>
<p>Potete verificare numericamente la correttezza dei risultati nella
slide precedente con questo programma Python, che estrae 100.000 numeri
casuali, li trasforma e ne traccia l’istogramma:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pylab <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>curplot <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_distribution(numbers, title, fun):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> curplot</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, curplot)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    plt.hist(fun(numbers), bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    curplot <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>numbers <span class="op">=</span> np.random.rand(<span class="dv">100_000</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>plot_distribution(numbers, <span class="st">&quot;$f(x) = x + 1$&quot;</span>, <span class="kw">lambda</span> x: x <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>plot_distribution(numbers, <span class="st">&quot;$f(x) = 2x$&quot;</span>, <span class="kw">lambda</span> x: <span class="dv">2</span> <span class="op">*</span> x)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plot_distribution(numbers, <span class="st">&quot;$f(x) = x^2$&quot;</span>, <span class="kw">lambda</span> x: x<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>plot_distribution(numbers, <span class="st">&quot;$f(x) = e^x$&quot;</span>, <span class="kw">lambda</span> x: np.exp(x))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&quot;distributions-python.svg&quot;</span>, bbox_inches<span class="op">=</span><span class="st">&quot;tight&quot;</span>)</span></code></pre></div>
</section>
<section class="slide level1">

<center>
<img data-src="./media/distributions-python.svg" />
</center>
</section>
<section id="distribuzioni-di-probabilità-2d" class="slide level1">
<h1>Distribuzioni di probabilità 2D</h1>
<ul>
<li><p>Nel caso si debbano campionare angoli solidi, il problema è più
complicato perché bisogna estrarre <em>coppie</em> di numeri
casuali.</p></li>
<li><p>Fortunatamente la matematica è abbastanza simile a quella vista
nel caso 1D; siccome sono in gioco integrali e cambi di variabile, è
scontato che nell’espressione debba comparire il determinante della
Jacobiana:</p>
<p><span class="math display">
p_Y(\vec y) = p_X\bigl(\vec{f}^{-1}(\vec y)\bigr)\cdot\left|\frac1{\det
J(y)}\right|.
</span></p></li>
</ul>
</section>
<section id="coordinate-polari" class="slide level1">
<h1>Coordinate polari</h1>
<ul>
<li><p>Supponiamo di estrarre due numeri <span class="math inline">r,
\theta</span> con una probabilità <span class="math inline">p(r,
\theta)</span>. Se <span class="math inline">r, \theta</span> sono le
coordinate polari di un numero sul piano, allora il punto ha
coordinate</p>
<p><span class="math display">
x = r \cos \theta, \quad y = r \sin \theta.
</span></p></li>
<li><p>Il determinante della matrice Jacobiana è</p>
<p><span class="math display">
\det J = \det\begin{pmatrix}
\partial_r x&amp; \partial_\theta x\\
\partial_r y&amp; \partial_\theta y
\end{pmatrix} =
\det\begin{pmatrix}
\cos\theta&amp; -r\sin\theta\\
\sin\theta&amp; r\cos\theta
\end{pmatrix} = r,
</span></p>
<p>e di conseguenza vale <span class="math inline">p(x, y) = p(r,
\theta) / r</span>, ossia <span class="math inline">p(r, \theta) =
r\cdot p(x, y)</span>.</p></li>
</ul>
</section>
<section id="coordinate-sferiche" class="slide level1">
<h1>Coordinate sferiche</h1>
<ul>
<li><p>Nel caso delle coordinate sferiche, vale che</p>
<p><span class="math display">
x = r \sin\theta\cos\varphi, \quad y = r \sin\theta\sin\varphi, \quad z
= r \cos\theta,
</span></p>
<p>e si ricava che <span class="math inline">\det J = r^2
\sin\theta</span>.</p></li>
<li><p>Di conseguenza, vale la relazione</p>
<p><span class="math display">
p(r, \theta, \varphi) = r^2 \sin\theta \cdot p(x, y, z),
</span></p>
<p>che come nel caso polare può essere usata sia per ricavare <span
class="math inline">p(x, y, z)</span> da <span class="math inline">p(r,
\theta, \varphi)</span> che viceversa.</p></li>
</ul>
</section>
<section id="campionare-la-semisfera" class="slide level1">
<h1>Campionare la semisfera</h1>
<ul>
<li><p>Torniamo al problema di estrarre direzioni casuali sulla
semisfera <span class="math inline">2\pi</span>. Questo equivale ad
estrarre coppie <span class="math inline">\theta, \varphi</span> tali
che la probabilità sia uniforme.</p></li>
<li><p>Siccome i generatori di numeri casuali permettono di estrarre
<em>un solo numero alla volta</em>, dobbiamo seguire una strada per
ricavare prima l’uno e poi l’altro.</p></li>
<li><p>Per spiegare il procedimento, abbiamo bisogno di due nuovi
concetti: la <em>funzione di densità marginale</em> e la <em>funzione di
densità condizionale</em>.</p></li>
</ul>
</section>
<section id="due-nuove-definizioni" class="slide level1">
<h1>Due nuove definizioni</h1>
<ul>
<li><p>Definiamo la <em>funzione di densità marginale</em> <span
class="math inline">p(x)</span>:</p>
<p><span class="math display">
p(x) = \int_\mathbb{R} p(x, y)\,\mathrm{d}y,
</span></p>
<p>che è la probabilità di ottenere <span class="math inline">x</span>
indipendentemente dal valore di <span
class="math inline">y</span>.</p></li>
<li><p>La <em>funzione di densità condizionale</em> <span
class="math inline">p(y | x)</span> è la probabilità di ottenere <span
class="math inline">y</span> nell’ipotesi che si sia ottenuto uno
specifico valore <span class="math inline">x</span>:</p>
<p><span class="math display">
p(y | x) = \frac{p(x, y)}{p(x)}.
</span></p></li>
</ul>
</section>
<section id="esempio-cornell-box" class="slide level1">
<h1>Esempio: «Cornell box»</h1>
<p><img data-src="./media/cornell_box_physical_model_image11.jpg"
height="560" /></p>
</section>
<section id="schema-del-cornell-box" class="slide level1">
<h1>Schema del «Cornell box»</h1>
<p><img data-src="./media/cornell-box-schema.svg" height="560" /></p>
</section>
<section id="esempio-4" class="slide level1">
<h1>Esempio</h1>
<ul>
<li><p>Facciamo un semplice esempio in cui <span
class="math inline">x</span> e <span class="math inline">y</span> sono
variabili discrete (Booleane) per capire le due definizioni.</p></li>
<li><p>Supponiamo che le due variabili <span
class="math inline">x</span> e <span class="math inline">y</span>
rappresentino questo:</p>
<ol>
<li><span class="math inline">x \in \{V, F\}</span> determina se un
raggio è partito dalla lampada sul soffitto;</li>
<li><span class="math inline">y \in \{V, F\}</span> determina se un
raggio colpisce il cubo di destra.</li>
</ol></li>
<li><p>Di conseguenza, <span class="math inline">p(V, V)</span> è la
probabilità che un raggio parta dalla lampada e raggiunga il cubo di
destra, <span class="math inline">p(F, V)</span> è la probabilità che un
raggio colpisca lo stesso cubo ma <strong>non</strong> sia partito dalla
lampada.</p></li>
</ul>
</section>
<section id="esempio-5" class="slide level1">
<h1>Esempio</h1>
<ul>
<li><p>La densità marginale <span class="math inline">p(x) = \int p(x,
y)\,\mathrm{d}y</span> nel nostro esempio si interpreta così: <span
class="math inline">p(V)</span> è la probabilità di che un raggio sia
partito dalla lampada sul soffitto, indipendentemente da dove sia
diretto.</p></li>
<li><p>La densità condizionale <span class="math inline">p(y | x)</span>
è tale per cui <span class="math inline">p(y = V | x = V)</span> dice
qual è la probabilità che un raggio partito dalla lampada (<span
class="math inline">x = V</span>) colpisca il cubo (<span
class="math inline">y = V</span>).</p></li>
<li><p>La densità condizionale <span class="math inline">p(x | y)</span>
(con <span class="math inline">x</span> e <span
class="math inline">y</span> scambiati) è tale per cui <span
class="math inline">p(y = V | x = V)</span> dice qual è la probabilità
che un raggio che ha colpito il cubo (<span class="math inline">y =
V</span>) sia partito dalla lampada (<span class="math inline">x =
V</span>).</p></li>
</ul>
</section>
<section id="applicazione-alle-direzioni" class="slide level1">
<h1>Applicazione alle direzioni</h1>
<ul>
<li><p>Vediamo ora come estrarre una direzione casuale sulla semisfera
2π usando i concetti appena appresi.</p></li>
<li><p>L’algoritmo è semplice:</p>
<ol>
<li>Calcoliamo la <em>densità marginale</em> di una delle due variabili,
ad esempio θ;</li>
<li>Estraiamo un valore casuale per θ secondo quella densità marginale:
è facile, perché ci siamo ricondotti a un caso 1D;</li>
<li>Una volta noto θ, usiamo la <em>densità condizionale</em> per
stimare la probabilità di ottenere φ dato il particolare θ che abbiamo
appena ottenuto;</li>
<li>Estraiamo φ seguendo la distribuzione appena ottenuta: anche qui
siamo in un caso monodimensionale semplice da trattare!</li>
</ol></li>
</ul>
</section>
<section id="applicazione-alle-direzioni-1" class="slide level1">
<h1>Applicazione alle direzioni</h1>
<ul>
<li><p>Se sulla semisfera <span class="math inline">\mathcal{H}^2</span>
deve valere che <span class="math inline">p(\omega) = c</span>,
allora</p>
<p><span class="math display">
\int_{\mathcal{H}^2} p(\omega)\,\mathrm{d}\omega = 1 \quad \Rightarrow
\quad c\int_{\mathcal{H}^2}\mathrm{d}\omega = 1\quad\Rightarrow\quad c =
\frac1{2\pi}.
</span></p></li>
<li><p>Siccome <span class="math inline">p(\omega) = 1 / (2\pi)</span> e
<span class="math inline">\mathrm{d}\omega =
\sin\theta\,\mathrm{d}\theta\,\mathrm{d}\varphi</span>, allora</p>
<p><span class="math display">
p(\omega)\mathrm{d}\omega = p(\theta,
\varphi)\,\mathrm{d}\theta\,\mathrm{d}\varphi\quad\Rightarrow\quad
p(\theta, \varphi) = \frac{\sin\theta}{2\pi}.
</span></p></li>
</ul>
</section>
<section id="pdf-di-θ-e-φ" class="slide level1">
<h1>PDF di θ e φ</h1>
<ul>
<li><p>La densità marginale <span class="math inline">p(\theta)</span> è
data da</p>
<p><span class="math display">
p(\theta) = \int_0^{2\pi} p(\theta, \varphi)\,\mathrm{d}\varphi =
\int_0^{2\pi} \frac{\sin\theta}{2\pi}\,\mathrm{d}\varphi = \sin\theta.
</span></p></li>
<li><p>La densità condizionale <span class="math inline">p(\varphi |
\theta)</span> è</p>
<p><span class="math display">
p(\varphi | \theta) = \frac{p(\theta, \varphi)}{p(\theta)} =
\frac1{2\pi}.
</span></p>
<p>Per φ quindi la PDF è costante, il che è sensato vista la simmetria
della variabile.</p></li>
</ul>
</section>
<section id="campionare-θ-e-φ" class="slide level1">
<h1>Campionare θ e φ</h1>
<ul>
<li><p>Per campionare θ e φ abbiamo bisogno della loro CDF, che è</p>
<p><span class="math display">
\begin{aligned}
P_\theta(\theta) &amp;= \int_0^\theta
\sin\theta&#39;\,\mathrm{d}\theta&#39; = 1 - \cos\theta,\\
P_\varphi(\varphi | \theta) &amp;= \int_0^\varphi
\frac1{2\pi}\,\mathrm{d}\varphi&#39; = \frac{\varphi}{2\pi}.
\end{aligned}
</span></p></li>
<li><p>Date due variabili <span class="math inline">X_1, X_2</span>
distribuite su <span class="math inline">[0, 1]</span>, le variabili θ e
φ che corrispondono alle CDF appena calcolate sono</p>
<p><span class="math display">
\theta = P_\theta^{-1}(X_1) = \arccos (1 - X_1) = \arccos X&#39;_1,\quad
\varphi = P_\varphi^{-1}(X_2) = 2\pi X_2.
</span></p></li>
</ul>
</section>
<section id="direzioni-casuali-1" class="slide level1">
<h1>Direzioni casuali</h1>
<p>Questo codice Python genera una distribuzione di direzioni uniforme
sull’angolo solido 2π:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pylab <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> np.random.rand(<span class="dv">1000</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> np.random.rand(<span class="bu">len</span>(x1))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Here is the transformation!</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>θ <span class="op">=</span> np.arccos(x1)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>φ <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> x2</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.sin(θ) <span class="op">*</span> np.cos(φ)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(θ) <span class="op">*</span> np.sin(φ)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.cos(θ)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(projection<span class="op">=</span><span class="st">&#39;3d&#39;</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>ax.scatter(x, y, z)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>ax.set_zlim(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&quot;uniform-density-random.svg&quot;</span>, bbox_inches<span class="op">=</span><span class="st">&quot;tight&quot;</span>)</span></code></pre></div>
</section>
<section class="slide level1">

<iframe src="pd-images/uniform-hemisphere-distribution.html" width="640" height="640" frameborder="0">
</iframe>
</section>
<section id="distribuzione-di-phong" class="slide level1">
<h1>Distribuzione di Phong</h1>
<ul>
<li><p>Una distribuzione più generale che ci servirà è la seguente:</p>
<p><span class="math display">
p(\omega) = k \cos^n\theta,
</span></p>
<p>con <span class="math inline">n</span> numero intero. (La forma che
abbiamo ottenuto in precedenza corrisponde al caso <span
class="math inline">n = 0</span>).</p></li>
<li><p>La normalizzazione si ottiene al solito modo:</p>
<p><span class="math display">
\int_{\mathcal{H}^2} k
\cos^n\theta\,\sin\theta\,\mathrm{d}\theta\,\mathrm{d}\varphi =
\frac{2\pi}{n + 1}\quad\Rightarrow\quad k = \frac{n + 1}{2\pi}.
</span></p></li>
</ul>
</section>
<section id="distribuzione-di-phong-1" class="slide level1">
<h1>Distribuzione di Phong</h1>
<ul>
<li><p>La densità marginale di <span class="math inline">\theta</span>
è</p>
<p><span class="math display">
p(\theta) = (n + 1) \cos^n\theta\,\sin\theta.
</span></p></li>
<li><p>La densità condizionale di <span
class="math inline">\varphi</span> è nuovamente una costante, com’è
evidente per la simmetria di <span
class="math inline">p(\omega)</span>:</p>
<p><span class="math display">
p(\varphi | \theta) = \frac1{2\pi}.
</span></p></li>
</ul>
</section>
<section id="risultato-di-phong" class="slide level1">
<h1>Risultato di Phong</h1>
<ul>
<li><p>Ripetendo i calcoli si ottiene</p>
<p><span class="math display">
\begin{aligned}
\theta &amp;= \arccos\left[\bigl(1 - X_1\bigr)^{\frac1{n + 1}}\right],\\
\varphi &amp;= 2\pi X_2,
\end{aligned}
</span></p>
<p>dove ancora una volta <span class="math inline">X_1</span> e <span
class="math inline">X_2</span> sono numeri casuali con distribuzione
uniforme su <span class="math inline">[0, 1]</span>.</p></li>
<li><p>Questa distribuzione <span class="math inline">p(\theta,
\varphi)</span> è chiamata <em>distribuzione di Phong</em>, e ci sarà
molto utile.</p></li>
</ul>
</section>
<section id="esempio-con-n-1" class="slide level1">
<h1>Esempio con <span class="math inline">n = 1</span></h1>
<iframe src="pd-images/cosine-hemisphere-distribution.html" width="560" height="560" frameborder="0">
</iframe>
</section>
<section id="brdf" class="slide level1">
<h1>BRDF</h1>
</section>
<section id="implementare-una-brdf" class="slide level1">
<h1>Implementare una BRDF</h1>
<ul>
<li><p>Per risolvere l’equazione del rendering bisogna valutare il
termine</p>
<p><span class="math display">
f_r(x, \Psi \rightarrow \Theta),
</span></p>
<p>che è un numero puro che «pesa» la quantità di radiazione proveniente
dalla direzione <span class="math inline">\Psi</span> e riflessa verso
<span class="math inline">\Theta</span>.</p></li>
<li><p>Siccome però <span class="math inline">f_r</span> dipende dalla
frequenza <span class="math inline">\lambda</span>, in realtà dovrebbe
essere codificato come una funzione <span class="math inline">f_r =
f_r(\lambda)</span>…</p></li>
<li><p>…ma per le proprietà dell’occhio umano ci basta che <span
class="math inline">f_r</span> restituisca <em>tre</em> valori: un
numero puro per la componente R, uno per G, e uno per B.</p></li>
</ul>
</section>
<section id="caratteristiche-della-brdf" class="slide level1">
<h1>Caratteristiche della BRDF</h1>
<ul>
<li><p>Concettualmente, è utile dal punto di vista del codice
considerare la BRDF di un materiale come composta da due tipi di
informazioni:</p>
<ol>
<li>Quelle proprietà che dipendono dall’angolo di incidenza della luce e
dalla posizione dell’osservatore;</li>
<li>Quelle proprietà che invece <strong>non</strong> dipendono dalla
direzione, e che vengono identificate sotto il nome collettivo di
<em>pigmento</em>.</li>
</ol></li>
<li><p>È comodo quindi definire un tipo <code>BRDF</code> che ha al suo
interno un sotto-tipo <code>Pigment</code>.</p></li>
</ul>
</section>
<section id="tipi-di-pigmenti" class="slide level1">
<h1>Tipi di pigmenti</h1>
<ul>
<li><p>Il pigmento <em>non</em> dice quale sia l’aspetto finale del
materiale, perché la maggiore o minore brillantezza/metallicità/… sono
definite dalla BRDF.</p></li>
<li><p>I pigmenti sono solitamente usati per rappresentare la
variabilità di una BRDF sulla superficie: sotto questa ipotesi, quello
che cambia da punto a punto non è <em>tutta</em> la BRDF, ma solo il
pigmento.</p></li>
<li><p>Nella <em>computer graphics</em> sono solitamente definiti vari
tipi di pigmenti:</p>
<ol>
<li>Uniforme (chiamato anche <em>solid</em>, chissà perché).</li>
<li>A scacchiera (<em>checkered</em>): molto utile per il
debugging.</li>
<li>Immagine (detto anche <em>textured</em>).</li>
<li><a
href="https://en.wikipedia.org/wiki/Procedural_texture">Procedurale</a>
(v. il capitolo 5 di Shirley &amp; Morley).</li>
</ol></li>
</ul>
</section>
<section class="slide level1">

<iframe src="https://player.vimeo.com/video/549664049?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" width="1102" height="620" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen title="Path-tracing example">
</iframe>
</section>
<section id="forma-della-brdf" class="slide level1">
<h1>Forma della BRDF</h1>
<ul>
<li><p>La dipendenza direzionale della BRDF dipende sia dall’angolo di
incidenza della luce rispetto alla normale <span
class="math inline">\hat n</span>, che dall’angolo di vista
dell’osservatore, sempre rispetto a <span class="math inline">\hat
n</span>.</p></li>
<li><p>Abbiamo già visto alcuni tipi di BRDF nella prima lezione:</p>
<ol>
<li><a
href="tomasi-ray-tracing-01a.html#/superficie-diffusiva-ideale">Superficie
diffusiva ideale</a>;</li>
<li><a
href="tomasi-ray-tracing-01a.html#/superficie-riflettente">Superficie
riflettente</a>.</li>
</ol></li>
<li><p>Nelle esercitazioni implementeremo le BRDF, insieme a un
generatore di numeri casuali.</p></li>
</ul>
</section>
<section id="esempio-6" class="slide level1">
<h1>Esempio</h1>
<center>
<img data-src="media/brdf-examples.webp" />
</center>
<p>Nella scena tutte le superfici sono diffusive ideali, tranne la sfera
rossa che implementa una BRDF riflettente. L’ambiente è una sfera di
raggio molto grande il cui materiale è basato su un <a
href="https://blog.gregzaal.com/2017/01/17/blender-institute-hdri/">file
HDR</a> (tramite un pigmento <em>textured</em>).</p>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@^4//dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="https://unpkg.com/reveal.js@^4//plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/zoom/zoom.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: true,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

       // Parallax background image
       parallaxBackgroundImage: './media/background.png', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1440,

        height: 810,

        // reveal.js plugins
        plugins: [
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script src="js/asciinema-player.js"></script>
    <script src="https://cdn.plot.ly/plotly-1.58.4.min.js"></script>
    <script type="text/javascript" src="./js/quantization.js"></script>
    </body>
</html>
